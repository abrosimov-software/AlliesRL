{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "website example. nvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_plus import MultiOutputPPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import gymnasium as gym\n",
    "import gymnasium_hybrid\n",
    "import time\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('Moving-v0', render_mode='rgb_array')\n",
    "\n",
    "# Instantiate the agent\n",
    "model = MultiOutputPPO(\n",
    "    policy='MultiOutputPolicy',\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    policy_kwargs=dict(\n",
    "        net_arch=dict(pi=[252] * 4, vf=[252] * 4)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Train the agent and display a progress bar\n",
    "model.learn(\n",
    "    total_timesteps=int(2e5),\n",
    "    progress_bar=True\n",
    ")\n",
    "\n",
    "# Save the agent\n",
    "model.save(\"ppo_moving\")\n",
    "del model  # delete trained model to demonstrate loading\n",
    "\n",
    "# Load the trained agent\n",
    "model = MultiOutputPPO.load(\"ppo_moving\", env=env)\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "print(f'Mean Reward {mean_reward} | Std Reward {std_reward}')\n",
    "\n",
    "# Enjoy trained agent\n",
    "vec_env = model.get_env()\n",
    "obs = vec_env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "    vec_env.render('human')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_plus import MultiOutputPPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "class UnoFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=256):\n",
    "        super(UnoFeatureExtractor, self).__init__(observation_space, features_dim)\n",
    "        self.layer = nn.Sequential(\n",
    "            nn.Linear(observation_space.shape[0], features_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features_dim, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, observations):\n",
    "        return self.layer(observations)\n",
    "\n",
    "env = UNOCommunicationEnv()  \n",
    "\n",
    "model = MultiOutputPPO(\n",
    "    policy=\"MultiOutputPolicy\",\n",
    "    env=env,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=UnoFeatureExtractor,\n",
    "        features_extractor_kwargs=dict(features_dim=256),\n",
    "        net_arch=dict(pi=[256, 256], vf=[256, 256])  \n",
    "    ),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "model.save(\"uno_model_based_agent\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extractor idea\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_plus import MultiOutputPPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "class UnoFeatureExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space, features_dim=256, message_dim=8):\n",
    "        super(UnoFeatureExtractor, self).__init__(observation_space, features_dim)\n",
    "        \n",
    "        self.message_dim = message_dim\n",
    "\n",
    "        # 1. last action + position\n",
    "        self.action_actor_layer = nn.Sequential(\n",
    "            nn.Linear(2, 64),  \n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # 2. availiable actions\n",
    "        self.available_actions_layer = nn.Sequential(\n",
    "            nn.Linear(62, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 3. cards on hand\n",
    "        self.cards_on_hand_layer = nn.Sequential(\n",
    "            nn.Linear(108, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 4. message + player position\n",
    "        self.message_layer = nn.Sequential(\n",
    "            nn.Linear(self.message_dim + 1, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # combined\n",
    "        self.combined_layer = nn.Sequential(\n",
    "            nn.Linear(64 * 4, features_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(features_dim, features_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, observations):\n",
    "\n",
    "        # 1. \n",
    "        last_action, actor_position = observations[0][0]\n",
    "        action_actor_features = self.action_actor_layer(th.tensor([last_action, actor_position]).float())\n",
    "\n",
    "        # 2. \n",
    "        available_actions = th.tensor(observations[0][1]).float()\n",
    "        available_actions_features = self.available_actions_layer(available_actions)\n",
    "\n",
    "        # 3.\n",
    "        cards_on_hand = th.tensor(observations[0][2]).float()\n",
    "        cards_on_hand_features = self.cards_on_hand_layer(cards_on_hand)\n",
    "\n",
    "        # 4. \n",
    "        messages = observations[0][3]\n",
    "        message_features = th.stack([\n",
    "            self.message_layer(th.cat([th.tensor(msg[0]).float(), th.tensor([msg[1]]).float()]))\n",
    "            for msg in messages\n",
    "        ]).sum(dim=0) \n",
    "\n",
    "        # combine\n",
    "        combined_features = th.cat([\n",
    "            action_actor_features, available_actions_features, cards_on_hand_features, message_features\n",
    "        ], dim=-1)\n",
    "        \n",
    "        return self.combined_layer(combined_features)\n",
    "    \n",
    "    \n",
    "\n",
    "env = UNOCommunicationEnv() \n",
    "\n",
    "model = MultiOutputPPO(\n",
    "    policy=\"MultiOutputPolicy\",\n",
    "    env=env,\n",
    "    policy_kwargs=dict(\n",
    "        features_extractor_class=UnoFeatureExtractor,\n",
    "        features_extractor_kwargs=dict(features_dim=256),\n",
    "        net_arch=dict(pi=[256, 256], vf=[256, 256])  \n",
    "    ),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "model.save(\"uno_based_agent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "step function should return observations, rewards, dones, infos\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
