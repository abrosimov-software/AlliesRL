{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "alpha = 0.1       \n",
    "gamma = 0.99      \n",
    "epsilon = 1.0     \n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_q_tables(n_agents, state_space, action_space):\n",
    "    q_tables = [np.zeros((state_space, action_space)) for _ in range(n_agents)]\n",
    "\n",
    "    return q_tables\n",
    "    \n",
    "\n",
    "def choose_action_with_positional_message(agent_idx, state, q_tables, action_space, n_agents, message_dim):\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.choice(action_space) \n",
    "    else:\n",
    "        action = np.argmax(q_tables[agent_idx][state])\n",
    "\n",
    "    # rotary position embedding?? or not here \n",
    "    content_vector = ...\n",
    "    positional_encoding = ...\n",
    "    message = (content_vector, positional_encoding)\n",
    "\n",
    "    return action, message\n",
    "\n",
    "\n",
    "def route_messages(messages):\n",
    "    routed_messages = env._route_messages(messages)\n",
    "    return routed_messages\n",
    "\n",
    "def update_q_value(q_table, state, action, reward, next_state):\n",
    "    best_next_action = np.argmax(q_table[next_state])\n",
    "    q_table[state, action] = q_table[state, action] + alpha * (\n",
    "        reward + gamma * q_table[next_state, best_next_action] - q_table[state, action]\n",
    "    )\n",
    "\n",
    "def train_multi_agent_with_position(env, q_tables, episodes, n_agents, message_dim):\n",
    "    global epsilon\n",
    "    for episode in range(episodes):\n",
    "        states = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            actions = []\n",
    "            messages = []\n",
    "            for agent_idx in range(n_agents):\n",
    "                action, message = choose_action_with_positional_message(\n",
    "                    agent_idx, states[agent_idx], q_tables, env.action_space[agent_idx].n, n_agents, message_dim\n",
    "                )\n",
    "                actions.append(action)\n",
    "                messages.append(message)\n",
    "\n",
    "            received_actions = tuple((actions[i], messages[i]) for i in range(n_agents))\n",
    "            \n",
    "            next_states, rewards, done, _ = env.step(actions)\n",
    "            routed_messages = env._route_messages(received_actions)  # Corrected format for `_route_messages`\n",
    "\n",
    "            for agent_idx in range(n_agents):\n",
    "                update_q_value(\n",
    "                    q_tables[agent_idx], states[agent_idx], actions[agent_idx], rewards[agent_idx], next_states[agent_idx]\n",
    "                )\n",
    "            \n",
    "            states = next_states\n",
    "        \n",
    "        epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "env = UNOCommunicationEnv(n_agents=..., message_dim=...)\n",
    "state_space = ...\n",
    "action_space = ...\n",
    "\n",
    "q_tables = initialize_q_tables(n_agents=4, state_space=state_space, action_space=action_space)\n",
    "\n",
    "train_multi_agent_with_position(env, q_tables, episodes=1000, n_agents=4, message_dim=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
