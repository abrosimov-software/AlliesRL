{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from gymnasium import spaces\n",
    "from typing import List, Tuple, Dict, Any, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNOGameEnv(gym.Env):\n",
    "    def __init__(self, n_agents: int):\n",
    "        # Environment configuration parameters\n",
    "        self.n_agents = n_agents\n",
    "        self.n_cards_total = 108\n",
    "        self.n_actions = 62\n",
    "        self.color_map = {'r': 0, 'g': 1, 'b': 2, 'y': 3, 'unknown': 4}\n",
    "        self.card_types_map = {\n",
    "            **{f'{i}': i for i in range(10)},\n",
    "            \"skip\": 10,\n",
    "            \"reverse\": 11,\n",
    "            \"draw2\": 12,\n",
    "            \"wild-default\": 13,\n",
    "            \"wild-draw4\": 14\n",
    "        }\n",
    "        self.action_types_map = {\n",
    "            **self.card_types_map,\n",
    "            \"draw\": 15,\n",
    "            \"pass\": 16\n",
    "        }\n",
    "        self.card_specs = self._get_card_specs()\n",
    "        self.actions_specs = self._get_actions_specs(self.card_specs)\n",
    "\n",
    "        # Rewards\n",
    "        self.invalid_action_penalty = -1\n",
    "        self.winning_reward = 1\n",
    "\n",
    "        # Action space\n",
    "        self.action_space = spaces.Tuple([\n",
    "            spaces.Discrete(self.n_actions) for _ in range(self.n_agents)\n",
    "        ])\n",
    "\n",
    "\n",
    "        # Observation space\n",
    "        self.public_observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(self.n_agents), # number of players\n",
    "            spaces.Discrete(4),  # game color\n",
    "            spaces.Discrete(15),  # played card type\n",
    "            spaces.Discrete(2),   # game direction (0 or 1)\n",
    "            spaces.Discrete(self.n_agents),  # current player\n",
    "            spaces.Discrete(self.n_cards_total + 1),  # number of cards in deck\n",
    "            spaces.MultiDiscrete([self.n_cards_total + 1] * self.n_agents)  # cards in hand for each player\n",
    "        ))\n",
    "        self.private_observation_space = spaces.Tuple((\n",
    "            spaces.MultiBinary(self.n_cards_total),    # cards in hand\n",
    "            spaces.Discrete(self.n_agents)            # agent's index\n",
    "        ))\n",
    "        self.observation_space = spaces.Tuple([\n",
    "            spaces.Tuple((\n",
    "                self.public_observation_space,\n",
    "                self.private_observation_space\n",
    "            )) for _ in range(self.n_agents)\n",
    "        ])\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    \n",
    "    def reset(self) -> List[Tuple]:\n",
    "        \"\"\"\n",
    "        Reset the environment.\n",
    "        \n",
    "        :returns: List of observations\n",
    "        \"\"\"\n",
    "        self.game_state = self.default_game_state()\n",
    "        return self._get_observations()\n",
    "\n",
    "    \n",
    "    def default_game_state(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Create a default game state.\n",
    "        \n",
    "        :returns: Default game state\n",
    "        \"\"\"\n",
    "        deck = np.random.permutation(self.n_cards_total).tolist()\n",
    "\n",
    "        hands = [np.zeros(self.n_cards_total) for _ in range(self.n_agents)]\n",
    "        for i in range(self.n_agents):\n",
    "            hands[i][deck[:7]] = 1\n",
    "            deck = deck[7:]\n",
    "            \n",
    "        current_agent = np.random.randint(self.n_agents)\n",
    "        game_direction = 1\n",
    "\n",
    "        discard_pile = [deck.pop()]\n",
    "        top_card_info = self.card_specs[discard_pile[-1]]\n",
    "        game_color = top_card_info[1] if top_card_info[1] != self.color_map['unknown'] else np.random.randint(4)\n",
    "        played_card_type = top_card_info[2]\n",
    "        if played_card_type == self.card_types_map[\"skip\"]:\n",
    "            current_agent = (current_agent + 1) % self.n_agents\n",
    "        elif played_card_type == self.card_types_map[\"reverse\"]:\n",
    "            game_direction *= -1\n",
    "        elif played_card_type == self.card_types_map[\"draw2\"]:\n",
    "            hands[current_agent][deck[:2]] = 1\n",
    "            deck = deck[2:]\n",
    "        elif played_card_type == self.card_types_map[\"wild-draw4\"]:\n",
    "            hands[current_agent][deck[:4]] = 1\n",
    "            deck = deck[4:]\n",
    "            game_color = np.random.randint(4)\n",
    "\n",
    "        return {\n",
    "            \"deck\": deck,\n",
    "            \"discard_pile\": discard_pile,\n",
    "            \"current_agent\": current_agent,\n",
    "            \"game_color\": game_color,\n",
    "            \"game_direction\": game_direction,\n",
    "            \"played_card_type\": played_card_type,\n",
    "            \"hands\": hands\n",
    "        }\n",
    "\n",
    "\n",
    "    def _get_observations(self) -> List[Tuple]:\n",
    "        \"\"\"\n",
    "        Get the observations for all agents.\n",
    "        \n",
    "        :returns: List of observations\n",
    "        \"\"\"\n",
    "        public_observation = self._get_public_observation()\n",
    "        return [\n",
    "            (public_observation, self._get_private_observation(i))\n",
    "            for i in range(self.n_agents)\n",
    "        ]\n",
    "    \n",
    "\n",
    "    def _get_public_observation(self) -> tuple:\n",
    "        \"\"\"\n",
    "        Get the public observation.\n",
    "        \n",
    "        :returns: Public observation\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.n_agents, # number of players\n",
    "            self.game_state[\"game_color\"],  # game color\n",
    "            self.game_state[\"played_card_type\"],  # played card type\n",
    "            1 if self.game_state[\"game_direction\"] == 1 else 0,  # game direction\n",
    "            self.game_state[\"current_agent\"],  # current player\n",
    "            len(self.game_state[\"deck\"]),  # number of cards in deck\n",
    "            [sum(hand) for hand in self.game_state[\"hands\"]]  # cards in hand for each player\n",
    "        )\n",
    "\n",
    "    \n",
    "    def _get_private_observation(self, agent_index: int) -> tuple:\n",
    "        \"\"\"\n",
    "        Get the private observation for an agent.\n",
    "        \n",
    "        :param agent_index: Index of the agent\n",
    "        :returns: Private observation\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.game_state[\"hands\"][agent_index],  # cards in hand\n",
    "            agent_index  # agent's index\n",
    "        )\n",
    "\n",
    "\n",
    "    def step(self, actions: List[int]) -> Tuple[List[Tuple], List[float], bool, Dict]:\n",
    "        \"\"\"\n",
    "        Executes a step in the environment.\n",
    "\n",
    "        :param actions: List of action IDs from all agents.\n",
    "        :return: A tuple containing observations, rewards, done flag, and info dictionary.\n",
    "        \"\"\"\n",
    "        rewards = [0 for _ in range(self.n_agents)]\n",
    "        done = False\n",
    "        info = {}\n",
    "\n",
    "        active_agent = self.game_state[\"current_agent\"]\n",
    "        active_action = actions[active_agent]\n",
    "\n",
    "        # Inactive agents must pass\n",
    "        for i, action in enumerate(actions):\n",
    "            if i != active_agent:\n",
    "                if self.actions_specs[action][2] != self.action_types_map[\"pass\"]:\n",
    "                    rewards[i] += self.invalid_action_penalty\n",
    "                \n",
    "        action_name, action_color, action_type, associated_cards_ids = self.actions_specs[active_action]\n",
    "        if action_type == self.action_types_map[\"pass\"]:\n",
    "            rewards[active_agent] += self.invalid_action_penalty\n",
    "        elif action_type == self.action_types_map[\"draw\"]:\n",
    "            if self.can_draw(1):\n",
    "                self.game_state[\"hands\"][active_agent][self.draw_cards(1)] = 1\n",
    "            else:\n",
    "                rewards[active_agent] += self.invalid_action_penalty\n",
    "                done = True\n",
    "        elif action_type == self.action_types_map[\"wild-draw4\"]:\n",
    "            next_agent = (active_agent + self.game_state[\"game_direction\"]) % self.n_agents\n",
    "            self.game_state[\"hands\"][next_agent][self.draw_cards(4)] = 1\n",
    "            self.game_state[\"game_color\"] = action_color\n",
    "        elif action_type == self.action_types_map[\"wild-default\"]:\n",
    "            self.game_state[\"game_color\"] = action_color\n",
    "        else:\n",
    "            if (self.game_state[\"game_color\"] == action_color or\n",
    "                self.game_state[\"played_card_type\"] == action_type):\n",
    "                card_to_play = -1\n",
    "                for card_id in associated_cards_ids:\n",
    "                    if self.game_state[\"hands\"][active_agent][card_id] == 1:\n",
    "                        card_to_play = card_id\n",
    "                        break\n",
    "                if card_to_play != -1:\n",
    "                    self.game_state[\"discard_pile\"].append(card_to_play)\n",
    "                    self.game_state[\"hands\"][active_agent][card_to_play] = 0\n",
    "                    self.game_state[\"played_card_type\"] = action_type\n",
    "                    self.game_state[\"game_color\"] = action_color\n",
    "                    if action_type == self.card_types_map[\"skip\"]:\n",
    "                        self.game_state[\"current_agent\"] = (active_agent + self.game_state[\"game_direction\"]) % self.n_agents\n",
    "                    elif action_type == self.card_types_map[\"reverse\"]:\n",
    "                        self.game_state[\"game_direction\"] *= -1\n",
    "                    elif action_type == self.card_types_map[\"draw2\"]:\n",
    "                        next_agent = (active_agent + self.game_state[\"game_direction\"]) % self.n_agents\n",
    "                        self.game_state[\"hands\"][next_agent][self.draw_cards(2)] = 1\n",
    "                else:\n",
    "                    rewards[active_agent] += self.invalid_action_penalty\n",
    "            else:\n",
    "                rewards[active_agent] += self.invalid_action_penalty\n",
    "\n",
    "        if self.game_state[\"hands\"][active_agent].sum() == 0:\n",
    "            done = True\n",
    "            rewards[active_agent] += self.winning_reward\n",
    "\n",
    "        self.game_state[\"current_agent\"] = (active_agent + self.game_state[\"game_direction\"]) % self.n_agents\n",
    "\n",
    "        return self._get_observations(), rewards, done, info\n",
    "\n",
    "\n",
    "    def can_draw(self, n: int) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the deck has enough cards to draw.\n",
    "\n",
    "        :param n: Number of cards to draw\n",
    "        :return: True if the deck has enough cards, False otherwise.\n",
    "        \"\"\"\n",
    "        return len(self.game_state[\"deck\"]) + len(self.game_state[\"discard_pile\"]) >= n\n",
    "    \n",
    "\n",
    "    def draw_cards(self, n: int) -> List[int]:\n",
    "        \"\"\"\n",
    "        Draw n cards from the deck.\n",
    "\n",
    "        :param n: Number of cards to draw\n",
    "        :return: List of card IDs\n",
    "        \"\"\"\n",
    "        if not self.can_draw(n):\n",
    "            ret = self.game_state[\"deck\"] + self.game_state[\"discard_pile\"]\n",
    "            self.game_state[\"deck\"] = []\n",
    "            self.game_state[\"discard_pile\"] = []\n",
    "            return ret\n",
    "        if len(self.game_state[\"deck\"]) < n:\n",
    "            self.game_state[\"deck\"] += np.random.permutation(self.game_state[\"discard_pile\"]).tolist()\n",
    "            self.game_state[\"discard_pile\"] = []\n",
    "        ret = self.game_state[\"deck\"][:n]\n",
    "        self.game_state[\"deck\"] = self.game_state[\"deck\"][n:]\n",
    "        return ret\n",
    "    \n",
    "    \n",
    "    def _get_card_specs(self) -> List[tuple]:\n",
    "        \"\"\" Get the card specifications. \"\"\"\n",
    "        card_specs = []\n",
    "        for color, color_id in self.color_map.items():\n",
    "            if color == 'unknown':\n",
    "                continue\n",
    "            for number in range(10):\n",
    "                card_specs.append((\n",
    "                    f\"{color}-{number}\",\n",
    "                    color_id,\n",
    "                    self.card_types_map[str(number)]\n",
    "                ))\n",
    "                if number > 0:\n",
    "                    card_specs.append((\n",
    "                        f\"{color}-{number}\",\n",
    "                        color_id,\n",
    "                        self.card_types_map[str(number)]\n",
    "                    ))\n",
    "            card_specs.extend([(f\"{color}-skip\", color_id, self.card_types_map[\"skip\"])] * 2)\n",
    "            card_specs.extend([(f\"{color}-reverse\", color_id, self.card_types_map[\"reverse\"])] * 2)\n",
    "            card_specs.extend([(f\"{color}-draw2\", color_id, self.card_types_map[\"draw2\"])] * 2)\n",
    "\n",
    "        card_specs.extend([(\"wild-default\", 4, self.card_types_map[\"wild-default\"])] * 4)\n",
    "        card_specs.extend([(\"wild-draw4\", 4, self.card_types_map[\"wild-draw4\"])] * 4)\n",
    "        return card_specs\n",
    "\n",
    "\n",
    "    def _get_actions_specs(self, cards_specs: List[tuple]) -> List[tuple]:\n",
    "        \"\"\" Get the action specifications. \"\"\"\n",
    "        actions = [] # [(action_name, action_color, action_type, associated_cards_ids)]\n",
    "        for color, color_id in self.color_map.items():\n",
    "            if color == 'unknown':\n",
    "                actions.extend([\n",
    "                    (\"draw\", color_id, self.action_types_map[\"draw\"], []),\n",
    "                    (\"pass\", color_id, self.action_types_map[\"pass\"], [])\n",
    "                ])\n",
    "                break\n",
    "            for number in range(10):\n",
    "                actions.append((\n",
    "                    f\"{color}-{number}\",\n",
    "                    color_id,\n",
    "                    self.card_types_map[str(number)],\n",
    "                    [i for i, card in enumerate(cards_specs) if card[1] == color_id and card[2] == number]\n",
    "                ))\n",
    "\n",
    "            actions.extend([\n",
    "                (f\"{color}-skip\", color_id, self.action_types_map[\"skip\"], [i for i, card in enumerate(cards_specs) if card[1] == color_id and card[2] == self.card_types_map[\"skip\"]]),\n",
    "                (f\"{color}-reverse\", color_id, self.action_types_map[\"reverse\"], [i for i, card in enumerate(cards_specs) if card[1] == color_id and card[2] == self.card_types_map[\"reverse\"]]),\n",
    "                (f\"{color}-draw2\", color_id, self.action_types_map[\"draw2\"], [i for i, card in enumerate(cards_specs) if card[1] == color_id and card[2] == self.card_types_map[\"draw2\"]]),\n",
    "                (f\"{color}-wild-default\", color_id, self.action_types_map[\"wild-default\"], [i for i, card in enumerate(cards_specs) if card[2] == self.card_types_map[\"wild-default\"]]),\n",
    "                (f\"{color}-wild-draw4\", color_id, self.action_types_map[\"wild-draw4\"], [i for i, card in enumerate(cards_specs) if card[2] == self.card_types_map[\"wild-draw4\"]])\n",
    "            ])\n",
    "\n",
    "        \n",
    "\n",
    "        return actions                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 2 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 3 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 4 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 5 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 6 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 7 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 8 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 9 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 10 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 11 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 12 finished. Rewards: [-1, -1, 0, -1, -1]\n",
      "Episode 13 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 14 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 15 finished. Rewards: [-1, 0, -1, -1, -1]\n",
      "Episode 16 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 17 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 18 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 19 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 20 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 21 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 22 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 23 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 24 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 25 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 26 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 27 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 28 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 29 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 30 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 31 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 32 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 33 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 34 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 35 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 36 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 37 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 38 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 39 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 40 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 41 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 42 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 43 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 44 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 45 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 46 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 47 finished. Rewards: [-1, -1, -1, 0, -1]\n",
      "Episode 48 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 49 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 50 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 51 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 52 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 53 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 54 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 55 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 56 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 57 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 58 finished. Rewards: [-1, 0, -1, -1, -1]\n",
      "Episode 59 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 60 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 61 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 62 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 63 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 64 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 65 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 66 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 67 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 68 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 69 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 70 finished. Rewards: [-1, -1, 0, -1, -1]\n",
      "Episode 71 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 72 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 73 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 74 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 75 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 76 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 77 finished. Rewards: [-1, 0, -1, -1, -1]\n",
      "Episode 78 finished. Rewards: [0, -1, -1, -1, -1]\n",
      "Episode 79 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 80 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 81 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 82 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 83 finished. Rewards: [-1, -1, -1, -1, 0]\n",
      "Episode 84 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 85 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 86 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 87 finished. Rewards: [-1, 0, -1, -1, -1]\n",
      "Episode 88 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 89 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 90 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 91 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 92 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 93 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 94 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 95 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 96 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 97 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 98 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 99 finished. Rewards: [-1, -1, -1, -1, -1]\n",
      "Episode 100 finished. Rewards: [-1, -1, -1, -1, -1]\n"
     ]
    }
   ],
   "source": [
    "env = UNOGameEnv(n_agents=5)\n",
    "num_episodes = 100\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    observations = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        actions = []\n",
    "        for obs in observations:\n",
    "            # Each agent selects a random valid action\n",
    "            agent_actions = list(range(env.n_actions))\n",
    "            action = np.random.choice(agent_actions)\n",
    "            actions.append(action)\n",
    "        observations, rewards, done, info = env.step(actions)\n",
    "    print(f\"Episode {episode+1} finished. Rewards: {rewards}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Helper Functions\n",
    "def relative_position_encoding(current_player, agents, n_agents):\n",
    "    distances = agents - current_player\n",
    "    theta = (distances / n_agents) * 2 * np.pi\n",
    "    return np.stack((np.sin(theta), np.cos(theta)), axis=-1)  # [n_agents, 2]\n",
    "\n",
    "def scale_card_count(card_count, max_card_count=108):\n",
    "    return np.log1p(card_count) / np.log1p(max_card_count)\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, n_cards_total, embed_dim):\n",
    "        super(FeatureExtractor, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.n_cards_total = n_cards_total\n",
    "\n",
    "        # Embedding layers for categorical features\n",
    "        self.color_embedding = nn.Embedding(4, embed_dim)  # Game color\n",
    "        self.card_type_embedding = nn.Embedding(15, embed_dim)  # Played card type\n",
    "        self.direction_embedding = nn.Embedding(2, embed_dim)  # Game direction\n",
    "\n",
    "        # Linear layers for numerical and binary features\n",
    "        self.numeric_embedding = nn.Linear(1, embed_dim)  # Deck count\n",
    "        self.card_hand_embedding = nn.Linear(n_cards_total, embed_dim)  # Private hand cards\n",
    "        self.card_counts_embedding = nn.Linear(1, embed_dim)  # Cards in hand count\n",
    "        self.rel_pos_fc = nn.Linear(2, embed_dim)  # Relative position encoding\n",
    "        self.agent_order_fc = nn.Linear(2, embed_dim)  # Agent order encoding\n",
    "\n",
    "        self.attention_layer = nn.Linear(embed_dim, 1)  # Compute attention scores\n",
    "\n",
    "    \n",
    "    def forward(self, public_obs, private_obs):\n",
    "        # Unpack public observations\n",
    "        n_agents = public_obs[0]\n",
    "        game_color = public_obs[1]\n",
    "        played_card_type = public_obs[2]\n",
    "        game_direction = public_obs[3]\n",
    "        current_player = public_obs[4]\n",
    "        deck_count = public_obs[5].float()\n",
    "        cards_in_hand_counts = public_obs[6].float()\n",
    "\n",
    "        # Unpack private observations\n",
    "        hand_cards = private_obs[0]\n",
    "        agent_index = private_obs[1]\n",
    "\n",
    "        # Process categorical features\n",
    "        color_emb = self.color_embedding(game_color)  # [embed_dim]\n",
    "        card_type_emb = self.card_type_embedding(played_card_type)  # [embed_dim]\n",
    "        direction_emb = self.direction_embedding(game_direction)  # [embed_dim]\n",
    "\n",
    "        # Process numerical features\n",
    "        deck_count_scaled = scale_card_count(deck_count)\n",
    "        deck_emb = self.numeric_embedding(deck_count_scaled.unsqueeze(-1))  # [embed_dim]\n",
    "\n",
    "        card_counts_scaled = scale_card_count(cards_in_hand_counts)\n",
    "        card_counts_emb = self.card_counts_embedding(card_counts_scaled.unsqueeze(-1))  # [n_agents, embed]\n",
    "\n",
    "        # Process private hand cards\n",
    "        hand_card_emb = self.card_hand_embedding(hand_cards)  # [embed_dim]\n",
    "\n",
    "        # Relative position encoding\n",
    "        agent_indices = torch.arange(n_agents)\n",
    "        rel_pos_enc = relative_position_encoding(agent_index, agent_indices, n_agents)  # [n_agents, 2]\n",
    "        rel_pos_emb = self.rel_pos_fc(torch.from_numpy(rel_pos_enc))  # [n_agents, embed_dim]\n",
    "        agent_order_enc = rel_pos_enc[current_player] # [2]\n",
    "        agent_order_emb = self.agent_order_fc(torch.from_numpy(agent_order_enc))  # [embed_dim]\n",
    "\n",
    "        # Combine all features\n",
    "        opponents_emb = card_counts_emb + rel_pos_emb # [n_agents, embed_dim]\n",
    "        # [n_agents + 6, embed_dim]\n",
    "        \n",
    "        all_features = torch.cat([\n",
    "            color_emb.unsqueeze(0), \n",
    "            card_type_emb.unsqueeze(0), \n",
    "            direction_emb.unsqueeze(0), \n",
    "            deck_emb.unsqueeze(0), \n",
    "            hand_card_emb.unsqueeze(0), \n",
    "            opponents_emb, \n",
    "            agent_order_emb.unsqueeze(0)\n",
    "            ])\n",
    "\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        attention_scores = self.attention_layer(all_features).squeeze(-1)  # [n_agents + 6]\n",
    "        attention_weights = F.softmax(attention_scores, dim=0)  # Normalize scores\n",
    "        attended_features = torch.sum(attention_weights.unsqueeze(-1) * all_features, dim=0)  # [embed_dim]\n",
    "\n",
    "        return attended_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_to_tensor(obs, device):\n",
    "    public_obs, private_obs = obs\n",
    "    public_obs = (\n",
    "        torch.tensor(public_obs[0], dtype=torch.long, device=device),\n",
    "        torch.tensor(public_obs[1], dtype=torch.long, device=device),\n",
    "        torch.tensor(public_obs[2], dtype=torch.long, device=device),\n",
    "        torch.tensor(public_obs[3], dtype=torch.long, device=device),\n",
    "        torch.tensor(public_obs[4], dtype=torch.long, device=device),\n",
    "        torch.tensor(public_obs[5], dtype=torch.float, device=device),\n",
    "        torch.tensor(public_obs[6], dtype=torch.float, device=device)\n",
    "    )\n",
    "    private_obs = (\n",
    "        torch.tensor(private_obs[0], dtype=torch.float, device=device),\n",
    "        torch.tensor(private_obs[1], dtype=torch.long, device=device)\n",
    "    )\n",
    "    return public_obs, private_obs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer:\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.logprobs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.is_terminals = []\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.feature_extractor = FeatureExtractor(n_cards_total=108, embed_dim=self.state_dim)\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
